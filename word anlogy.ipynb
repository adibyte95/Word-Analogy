{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading the glove word embedding matrix values\n",
    "def load_glove_vectors(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as file:\n",
    "        # unique words\n",
    "        words = set()\n",
    "        word_to_vec = {}\n",
    "        # each line starts with a word then the values for the different features\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            # take the word \n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            # rest of the features for the word\n",
    "            word_to_vec[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading the glove word embedding matrix values\n",
    "def load_glove_vectors(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as file:\n",
    "        # unique words\n",
    "        words = set()\n",
    "        word_to_vec = {}\n",
    "        # each line starts with a word then the values for the different features\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            # take the word \n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            # rest of the features for the word\n",
    "            word_to_vec[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec = load_glove_vectors('data/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cosine_similarity(u, v):\n",
    "    distance = 0.0\n",
    "    \n",
    "    # find the dot product between u and v \n",
    "    dot = np.dot(u,v)\n",
    "    # find the L2 norm of u \n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    # Compute the cosine similarity\n",
    "    distance = dot/(norm_u)/norm_v\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(king, queen) =  0.633646870148\n",
      "cosine_similarity(father, mother) =  0.756821737365\n",
      "cosine_similarity(king - queen, father - mother) =  0.471706450424\n",
      "cosine_similarity(bat, crow) =  0.151650597086\n",
      "cosine_similarity(india - delhi, rome - italy) =  -0.519746805621\n"
     ]
    }
   ],
   "source": [
    "father = word_to_vec[\"father\"]\n",
    "mother = word_to_vec[\"mother\"]\n",
    "king = word_to_vec[\"king\"]\n",
    "queen = word_to_vec[\"queen\"]\n",
    "bat = word_to_vec[\"bat\"]\n",
    "crow = word_to_vec[\"crow\"]\n",
    "india = word_to_vec[\"india\"]\n",
    "italy = word_to_vec[\"italy\"]\n",
    "delhi = word_to_vec[\"delhi\"]\n",
    "rome = word_to_vec[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(king, queen) = \", find_cosine_similarity(king, queen))\n",
    "print(\"cosine_similarity(father, mother) = \", find_cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(king - queen, father - mother) = \",find_cosine_similarity(king - queen, father - mother))\n",
    "print(\"cosine_similarity(bat, crow) = \",find_cosine_similarity(bat, crow))\n",
    "print(\"cosine_similarity(india - delhi, rome - italy) = \",find_cosine_similarity(india - delhi, rome - italy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the Word analogy task: a is to b as c is to ____\n",
    "def find_analogy(word_a, word_b, word_c, word_to_vec):\n",
    "    # convert words to lower case\n",
    "    word_a = word_a.lower()\n",
    "    word_b = word_b.lower()\n",
    "    word_c = word_c.lower()\n",
    "    \n",
    "    \n",
    "    # find the word embeddings for word_a, word_b, word_c\n",
    "    e_a, e_b, e_c = word_to_vec[word_a], word_to_vec[word_b], word_to_vec[word_c]\n",
    "    \n",
    "    words = word_to_vec.keys()\n",
    "    max_cosine_sim = -999              \n",
    "    best_word = None                  \n",
    "\n",
    "    # search for word_d in the whole word vector set\n",
    "    for w in words:        \n",
    "        # ignore input words\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "\n",
    "        # Compute cosine similarity between the vectors u and v\n",
    "        #u:(e_b - e_a) \n",
    "        #v:((w's vector representation) - e_c)\n",
    "        cosine_sim = find_cosine_similarity(e_b - e_a, word_to_vec[w] - e_c)\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            # update word_d\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india -> delhi :: japan -> tokyo\n",
      "tall -> taller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "examples = [('india', 'delhi', 'japan'), ('tall', 'taller', 'large')]\n",
    "for example in examples:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *example, find_analogy(*example, word_to_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for taking input from the user and doing word analogy task on that\n",
    "def take_input():\n",
    "    print('a --> b :: c --> d')\n",
    "    print('Enter a, b, c words separated by space')\n",
    "    words = input().split(' ')\n",
    "    \n",
    "    best_pick = find_analogy(*words, word_to_vec)\n",
    "    print ('{} -> {} :: {} -> {}'.format( *words, best_pick))\n",
    "    print('Best pick: ' + best_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a --> b :: c --> d\n",
      "Enter a, b, c words separated by space\n",
      "man king women\n",
      "man -> king :: women -> queen\n",
      "Best pick: queen\n"
     ]
    }
   ],
   "source": [
    "take_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a --> b :: c --> d\n",
      "Enter a, b, c words separated by space\n",
      "india delhi pakistan\n",
      "india -> delhi :: pakistan -> islamabad\n",
      "Best pick: islamabad\n"
     ]
    }
   ],
   "source": [
    "take_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a --> b :: c --> d\n",
      "Enter a, b, c words separated by space\n",
      "hindu diwali muslim\n",
      "hindu -> diwali :: muslim -> ul-fitr\n",
      "Best pick: ul-fitr\n"
     ]
    }
   ],
   "source": [
    "take_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

    "    return words, word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec = load_glove_vectors('data/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cosine_similarity(u, v):\n",
    "    distance = 0.0\n",
    "    \n",
    "    # find the dot product between u and v \n",
    "    dot = np.dot(u,v)\n",
    "    # find the L2 norm of u \n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    # Compute the cosine similarity\n",
    "    distance = dot/(norm_u)/norm_v\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(king, queen) =  0.633646870148\n",
      "cosine_similarity(father, mother) =  0.756821737365\n",
      "cosine_similarity(king - queen, father - mother) =  0.471706450424\n",
      "cosine_similarity(bat, crow) =  0.151650597086\n",
      "cosine_similarity(india - delhi, rome - italy) =  -0.519746805621\n"
     ]
    }
   ],
   "source": [
    "father = word_to_vec[\"father\"]\n",
    "mother = word_to_vec[\"mother\"]\n",
    "king = word_to_vec[\"king\"]\n",
    "queen = word_to_vec[\"queen\"]\n",
    "bat = word_to_vec[\"bat\"]\n",
    "crow = word_to_vec[\"crow\"]\n",
    "india = word_to_vec[\"india\"]\n",
    "italy = word_to_vec[\"italy\"]\n",
    "delhi = word_to_vec[\"delhi\"]\n",
    "rome = word_to_vec[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(king, queen) = \", find_cosine_similarity(king, queen))\n",
    "print(\"cosine_similarity(father, mother) = \", find_cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(king - queen, father - mother) = \",find_cosine_similarity(king - queen, father - mother))\n",
    "print(\"cosine_similarity(bat, crow) = \",find_cosine_similarity(bat, crow))\n",
    "print(\"cosine_similarity(india - delhi, rome - italy) = \",find_cosine_similarity(india - delhi, rome - italy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the Word analogy task: a is to b as c is to ____\n",
    "def find_analogy(word_a, word_b, word_c, word_to_vec):\n",
    "    # convert words to lower case\n",
    "    word_a = word_a.lower()\n",
    "    word_b = word_b.lower()\n",
    "    word_c = word_c.lower()\n",
    "    \n",
    "    \n",
    "    # find the word embeddings for word_a, word_b, word_c\n",
    "    e_a, e_b, e_c = word_to_vec[word_a], word_to_vec[word_b], word_to_vec[word_c]\n",
    "    \n",
    "    words = word_to_vec.keys()\n",
    "    max_cosine_sim = -999              \n",
    "    best_word = None                  \n",
    "\n",
    "    # search for word_d in the whole word vector set\n",
    "    for w in words:        \n",
    "        # ignore input words\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "\n",
    "        # Compute cosine similarity between the vectors u and v\n",
    "        #u:(e_b - e_a) \n",
    "        #v:((w's vector representation) - e_c)\n",
    "        cosine_sim = find_cosine_similarity(e_b - e_a, word_to_vec[w] - e_c)\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            # update word_d\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india -> delhi :: japan -> tokyo\n",
      "tall -> taller :: large -> larger\n"
     ]
    }
   ],
   "source": [
    "examples = [('india', 'delhi', 'japan'), ('tall', 'taller', 'large')]\n",
    "for example in examples:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *example, find_analogy(*example, word_to_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for taking input from the user and doing word analogy task on that\n",
    "def take_input():\n",
    "    print('a --> b :: c --> d')\n",
    "    print('Enter a, b, c words separated by space')\n",
    "    words = input().split(' ')\n",
    "    \n",
    "    best_pick = find_analogy(*words, word_to_vec)\n",
    "    print ('{} -> {} :: {} -> {}'.format( *words, best_pick))\n",
    "    print('Best pick: ' + best_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a --> b :: c --> d\n",
      "Enter a, b, c words separated by space\n",
      "women bitch man\n",
      "women -> bitch :: man -> bastard\n",
      "Best pick: bastard\n"
     ]
    }
   ],
   "source": [
    "take_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
